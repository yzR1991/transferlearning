'''
This is to provide a copula-based loss function.
'''

import numpy as np
import mmd
import torch

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class COPULA():
    def __init__(self, lambda=1):
        self.lambda = lambda
    
    def marginal_loss(self, Xs, Xt):
        m_loss = 0
        
        return m_loss
    
    def copula_loss(self, Xs, Xt):
        '''
        As the KL-divergence of two covariances of normal distributions.
        '''

        d = Xs.size(1)
        ns, nt = Xs.size(0), Xt.size(0)
        
        tmp_s = torch.ones((1, ns)).to(DEVICE) @ Xs
        cs = (Xs.t() @ Xs - (tmp_s.t() @ tmp_s) / ns) / (ns - 1)
        
        tmp_t = torch.ones((1, nt)).to(DEVICE) @ Xt
        ct = (Xt.t() @ Xt - (tmp_t.t() @ tmp_t) / nt) / (nt - 1)
        
        c_loss = 0.5 * (torch.log(torch.det(cs) / torch.det(ct)) - d + torch.trace(ct @ torch.inverse(cs)))
        
        return c_loss
    
    def total_loss(self, Xs, Xt):
        return self.marginal_loss(Xs, Xt) + self.lambda * self.copula_loss(Xs,Xt)
